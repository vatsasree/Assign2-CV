{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsasree/CV-Assignment-2/blob/main/CV_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWTMbLVJatuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLDbm0bOgbgR",
        "outputId": "13f83692-3795-4661-9906-cc23a83f40b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 89479406.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35326828.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27456434.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6416479.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMrkNf5gfBe7",
        "outputId": "33956359-83d6-45ab-a6b8-10fd031ccb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mwgL67XfHEq",
        "outputId": "58f03419-fa08-46a3-ce8e-7d4a99907308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRQbaoZ1lVX5"
      },
      "outputs": [],
      "source": [
        "batch_size = 60\n",
        "n_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTVXhfA7lLDh"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataLoader(train_set,\n",
        "                           shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QncfcF6Qlj4o"
      },
      "outputs": [],
      "source": [
        "test_dataset = DataLoader(test_set,\n",
        "                           shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHkX681PIR49"
      },
      "outputs": [],
      "source": [
        "# def extract_sift_features(image):\n",
        "#     # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#     if image.dtype != np.uint8:\n",
        "#         image = (image * 255).astype(np.uint8)\n",
        "#     gray = image\n",
        "#     print(gray.shape, type(gray))\n",
        "#     sift = cv2.SIFT_create()\n",
        "#     keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "#     return keypoints, descriptors\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# def cluster_sift_features(descriptors, num_clusters):\n",
        "#     kmeans = KMeans(n_clusters=num_clusters)\n",
        "#     kmeans.fit(descriptors)\n",
        "#     return kmeans.cluster_centers_\n",
        "\n",
        "# def compute_histogram(keypoints, descriptors, codebook):\n",
        "#     # kmeans = KMeans(n_clusters=codebook.shape[0], init=codebook)\n",
        "#     kmeans = KMeans(n_clusters=num_clusters)\n",
        "#     kmeans.fit(descriptors)\n",
        "#     histogram = np.zeros(codebook.shape[0])\n",
        "#     for cluster_label in kmeans.labels_:\n",
        "#         histogram[cluster_label] += 1\n",
        "#     return histogram\n",
        "\n",
        "def compute_cluster_centers(descriptors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(descriptors)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "# Function to compute histogram of visual words for an image\n",
        "def compute_histogram(descriptors, cluster_centers):\n",
        "    histogram = np.zeros(len(cluster_centers))\n",
        "    for descriptor in descriptors:\n",
        "        distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
        "        nearest_cluster = np.argmin(distances)\n",
        "        histogram[nearest_cluster] += 1\n",
        "    return histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05sNy6IzJFuP"
      },
      "outputs": [],
      "source": [
        "# from sklearn import svm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Extract SIFT features, compute histograms, and build dataset\n",
        "# X = []\n",
        "# y = []\n",
        "\n",
        "# num_clusters = 100\n",
        "\n",
        "\n",
        "# for image, labels in train_dataset:\n",
        "#     print(image.shape)\n",
        "#     label = labels.item()\n",
        "#     image = image[0].permute(1, 2, 0).numpy()\n",
        "#     print(image.shape, type(image))\n",
        "#     # gray_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "#     # gray_image = image.astype(np.uint8)\n",
        "#     # gray_image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "#     keypoints, descriptors = extract_sift_features(image)\n",
        "#     print(len(keypoints), len(descriptors))\n",
        "#     codebook = cluster_sift_features(descriptors, num_clusters)\n",
        "#     histogram = compute_histogram(keypoints, descriptors, codebook)\n",
        "#     X.append(histogram)\n",
        "#     y.append(label)\n",
        "\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "\n",
        "# # Split dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Train linear SVM model\n",
        "# svm_model = svm.LinearSVC()\n",
        "# svm_model.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRoKTznWK6Gn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Function to compute cluster centers using K-means\n",
        "def compute_cluster_centers(descriptors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(descriptors)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "# Function to compute histogram of visual words for an image\n",
        "def compute_histogram(descriptors, cluster_centers):\n",
        "    histogram = np.zeros(len(cluster_centers))\n",
        "    for descriptor in descriptors:\n",
        "        distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
        "        nearest_cluster = np.argmin(distances)\n",
        "        histogram[nearest_cluster] += 1\n",
        "    return histogram\n",
        "\n",
        "# # Load MNIST dataset\n",
        "# mnist = fetch_openml('mnist_784', version=1)\n",
        "# X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "def full(num_clusters):\n",
        "    # Initialize SIFT\n",
        "    X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # Initialize lists to store descriptors\n",
        "    descriptors_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        image = X_train[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        # descriptors_list.append(descriptors)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            descriptors_list.append(descriptors)\n",
        "            labels_list.append(y_train[i])  # Append corresponding label\n",
        "\n",
        "    # descriptors_list = [descriptors for descriptors in descriptors_list if descriptors is not None and descriptors.any()]\n",
        "\n",
        "    # Convert descriptors to a single numpy array\n",
        "    if descriptors_list:\n",
        "        descriptors_array = np.concatenate(descriptors_list, axis=0)\n",
        "\n",
        "        # Choose the number of clusters for K-means\n",
        "        num_clusters = num_clusters\n",
        "\n",
        "        # Compute cluster centers using K-means\n",
        "        cluster_centers = compute_cluster_centers(descriptors_array, num_clusters)\n",
        "\n",
        "        # Compute histograms of visual words for all images\n",
        "        t_histograms = []\n",
        "        t_labels = []\n",
        "        for descriptors, label in zip(descriptors_list, labels_list):\n",
        "            histogram = compute_histogram(descriptors, cluster_centers)\n",
        "            t_histograms.append(histogram)\n",
        "            t_labels.append(label)\n",
        "\n",
        "        # Convert histograms and labels to numpy arrays\n",
        "        X_train = np.array(t_histograms)\n",
        "        y_train = np.array(t_labels)\n",
        "\n",
        "        # Create a Linear SVM model\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
        "\n",
        "        # Train the SVM model\n",
        "        svm_model.fit(X_train, y_train)\n",
        "    else:\n",
        "        print(\"No descriptors found. Unable to create SVM model.\")\n",
        "\n",
        "    # Assuming you have a separate test set, similar to the training set\n",
        "    X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
        "\n",
        "    # Initialize lists to store test descriptors and labels\n",
        "    test_descriptors_list = []\n",
        "    test_labels_list = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        image = X_test[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            test_descriptors_list.append(descriptors)\n",
        "            test_labels_list.append(y_test[i])\n",
        "\n",
        "    # Compute histograms of visual words for all test images\n",
        "    test_histograms = []\n",
        "    test_labels = []\n",
        "    for descriptors, label in zip(test_descriptors_list, test_labels_list):\n",
        "        histogram = compute_histogram(descriptors, cluster_centers)\n",
        "        test_histograms.append(histogram)\n",
        "        test_labels.append(label)\n",
        "\n",
        "    # Convert test histograms and labels to numpy arrays\n",
        "    X_test = np.array(test_histograms)\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    # Make predictions using the trained SVM model\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy of the SVM model on the test set:\", accuracy)\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    filename = 'finalized_model_{}.sav'.format(num_clusters)\n",
        "    pickle.dump(svm_model, open(filename, 'wb'))\n",
        "\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHmkSCqzUdv",
        "outputId": "8c38685e-525c-426a-aef7-9898e4c08ad0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.44873366013071897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.5313521241830066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.6437908496732027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.7238562091503268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.7952410130718954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clusters=[10,20,40,80,160,320]\n",
        "accuracies=[]\n",
        "for num_clusters in clusters:\n",
        "  accuracy = full(num_clusters)\n",
        "  accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wJ11bXNui60"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# filename = 'finalized_model_2.sav'\n",
        "# pickle.dump(svm_model, open(filename, 'wb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvBYppUpb1jT"
      },
      "outputs": [],
      "source": [
        "# # Assuming you have a separate test set, similar to the training set\n",
        "# X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
        "\n",
        "# # Initialize lists to store test descriptors and labels\n",
        "# test_descriptors_list = []\n",
        "# test_labels_list = []\n",
        "\n",
        "# for i in range(len(X_test)):\n",
        "#     image = X_test[i]\n",
        "#     keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "#     if descriptors is not None and descriptors.any():\n",
        "#         test_descriptors_list.append(descriptors)\n",
        "#         test_labels_list.append(y_test[i])\n",
        "\n",
        "# # Compute histograms of visual words for all test images\n",
        "# test_histograms = []\n",
        "# test_labels = []\n",
        "# for descriptors, label in zip(test_descriptors_list, test_labels_list):\n",
        "#     histogram = compute_histogram(descriptors, cluster_centers)\n",
        "#     test_histograms.append(histogram)\n",
        "#     test_labels.append(label)\n",
        "\n",
        "# # Convert test histograms and labels to numpy arrays\n",
        "# X_test = np.array(test_histograms)\n",
        "# y_test = np.array(test_labels)\n",
        "\n",
        "# # Make predictions using the trained SVM model\n",
        "# y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# # Calculate the accuracy of the model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy of the SVM model on the test set:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "HIk_5H5rCi-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32tRBXMVDGjb",
        "outputId": "6361dd15-ae7c-4e7a-b280-bfc7fa223c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MIXtRUpu1QW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform = transforms.Compose([\n",
        "                                                  transforms.Resize((28,28)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform = transforms.Compose([\n",
        "                                                  transforms.Resize((28,28)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]))\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model_lenet = LeNet()\n"
      ],
      "metadata": {
        "id": "q1T_KQeHDhC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(1, hidden_dim, kernel_size=5, stride=2, padding=3)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, hidden_dim, 16, 16))\n",
        "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(hidden_dim, 8, hidden_dim * 4, 0.1), num_layers)\n",
        "        self.fc = nn.Linear(hidden_dim * 16 * 16, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)  # [batch_size, hidden_dim, 14, 14]\n",
        "        x = x + self.pos_embedding\n",
        "        x = x.flatten(2).permute(2, 0, 1)  # [14*14, batch_size, hidden_dim]\n",
        "        x = self.transformer_encoder(x)  # [14*14, batch_size, hidden_dim]\n",
        "        x = x.permute(1, 0, 2).flatten(1, 2)  # [batch_size, 14*14*hidden_dim]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "input_dim = 28*28\n",
        "hidden_dim = 64\n",
        "output_dim = 10\n",
        "num_layers = 2\n",
        "model_tr = TransformerEncoderModel(input_dim, hidden_dim, output_dim, num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ5OZxUlDs7p",
        "outputId": "612ec419-10d2-47cc-ddc3-34aed70d069a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_one_epoch(train_loader, model, optimizer, criterion, log_to_wandb):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=\"Training Batches\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100.0 * correct / total\n",
        "    if log_to_wandb:\n",
        "        wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy})\n",
        "    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "def test(model, test_loader, log_to_wandb):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Testing Batches\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100.0 * correct / total\n",
        "    if log_to_wandb:\n",
        "        wandb.log({\"test_accuracy\": test_accuracy})\n",
        "    return test_accuracy\n"
      ],
      "metadata": {
        "id": "d-r0os_1DMEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(model, train_sizes, log_to_wandb, name):\n",
        "    accuracies = []\n",
        "    batch_size = 64\n",
        "    for train_size in train_sizes:\n",
        "        accs = []\n",
        "        # name = \"trainsize_{}\".format(train_size)\n",
        "        name = name+'_{}'.format(train_size)\n",
        "        if log_to_wandb:\n",
        "            run = wandb.init(\n",
        "                name = name,\n",
        "                id = name,\n",
        "                project=\"CV-Assignment-2\",\n",
        "                config={\n",
        "                    \"learning_rate\": 0.01,\n",
        "                    \"epochs\": 10,\n",
        "                    \"batch_size\":batch_size\n",
        "                },\n",
        "            )\n",
        "\n",
        "        model = model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        train_subset, _ = torch.utils.data.random_split(train_dataset, [train_size, len(train_dataset) - train_size])\n",
        "        train_loader_subset = torch.utils.data.DataLoader(dataset=train_subset,\n",
        "                                                           batch_size=batch_size,\n",
        "                                                           shuffle=True)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            fit_one_epoch(train_loader_subset, model, optimizer, criterion, log_to_wandb)\n",
        "            test_accuracy = test(model, test_loader, log_to_wandb)\n",
        "            print(f'Test Accuracy for {epoch}: {test_accuracy:.2f}%')\n",
        "            accs.append(test_accuracy)\n",
        "        accuracies.append(accs[-1])\n",
        "    return accuracies\n",
        "\n",
        "# Experiment 1: Varying the number of training samples with CNN\n",
        "\n",
        "\n",
        "# # Experiment 2: Varying the number of training samples with Transformer Encoder\n",
        "# print(\"\\nTransformer Encoder Model:\")\n",
        "# train_sizes = [6000, 60000]\n",
        "# accuracies = train(TransformerEncoder(1, 64, 10, 2), train_sizes)\n",
        "\n",
        "# Plot the results\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(train_sizes, accuracies, marker='o')\n",
        "# plt.xlabel('Number of training samples')\n",
        "# plt.ylabel('Accuracy (%)')\n",
        "# plt.title('Classification accuracy vs. number of training samples')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Brb8rEVoDRAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN Model:\")\n",
        "train_sizes = [600, 1800, 6000, 18000, 60000]\n",
        "log_to_wandb = False\n",
        "name = 'model_tr'\n",
        "accuracies = main(model_tr, train_sizes, log_to_wandb, name)"
      ],
      "metadata": {
        "id": "sDnK1HdLDYjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}