{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsasree/CV-Assignment-2/blob/main/CV_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WWTMbLVJatuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLDbm0bOgbgR",
        "outputId": "a1b8365a-a8e0-4bd7-a872-5a985ce52d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 655360/9912422 [00:00<00:11, 829600.15it/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4331429.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 135963.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:02<00:00, 791448.13it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11936421.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMrkNf5gfBe7",
        "outputId": "2e71a6b2-c351-4fb9-9052-a41e790aa639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mwgL67XfHEq",
        "outputId": "ac22861e-c14e-45cb-e0c2-66596cc3b794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UTVXhfA7lLDh"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataLoader(train_set,\n",
        "                           shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QncfcF6Qlj4o"
      },
      "outputs": [],
      "source": [
        "test_dataset = DataLoader(test_set,\n",
        "                           shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eHkX681PIR49"
      },
      "outputs": [],
      "source": [
        "# def extract_sift_features(image):\n",
        "#     # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#     if image.dtype != np.uint8:\n",
        "#         image = (image * 255).astype(np.uint8)\n",
        "#     gray = image\n",
        "#     print(gray.shape, type(gray))\n",
        "#     sift = cv2.SIFT_create()\n",
        "#     keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "#     return keypoints, descriptors\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# def cluster_sift_features(descriptors, num_clusters):\n",
        "#     kmeans = KMeans(n_clusters=num_clusters)\n",
        "#     kmeans.fit(descriptors)\n",
        "#     return kmeans.cluster_centers_\n",
        "\n",
        "# def compute_histogram(keypoints, descriptors, codebook):\n",
        "#     # kmeans = KMeans(n_clusters=codebook.shape[0], init=codebook)\n",
        "#     kmeans = KMeans(n_clusters=num_clusters)\n",
        "#     kmeans.fit(descriptors)\n",
        "#     histogram = np.zeros(codebook.shape[0])\n",
        "#     for cluster_label in kmeans.labels_:\n",
        "#         histogram[cluster_label] += 1\n",
        "#     return histogram\n",
        "\n",
        "def compute_cluster_centers(descriptors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(descriptors)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "# Function to compute histogram of visual words for an image\n",
        "def compute_histogram(descriptors, cluster_centers):\n",
        "    histogram = np.zeros(len(cluster_centers))\n",
        "    for descriptor in descriptors:\n",
        "        distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
        "        nearest_cluster = np.argmin(distances)\n",
        "        histogram[nearest_cluster] += 1\n",
        "    return histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "05sNy6IzJFuP"
      },
      "outputs": [],
      "source": [
        "# from sklearn import svm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Extract SIFT features, compute histograms, and build dataset\n",
        "# X = []\n",
        "# y = []\n",
        "\n",
        "# num_clusters = 100\n",
        "\n",
        "\n",
        "# for image, labels in train_dataset:\n",
        "#     print(image.shape)\n",
        "#     label = labels.item()\n",
        "#     image = image[0].permute(1, 2, 0).numpy()\n",
        "#     print(image.shape, type(image))\n",
        "#     # gray_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "#     # gray_image = image.astype(np.uint8)\n",
        "#     # gray_image = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "#     keypoints, descriptors = extract_sift_features(image)\n",
        "#     print(len(keypoints), len(descriptors))\n",
        "#     codebook = cluster_sift_features(descriptors, num_clusters)\n",
        "#     histogram = compute_histogram(keypoints, descriptors, codebook)\n",
        "#     X.append(histogram)\n",
        "#     y.append(label)\n",
        "\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "\n",
        "# # Split dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Train linear SVM model\n",
        "# svm_model = svm.LinearSVC()\n",
        "# svm_model.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zRoKTznWK6Gn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Function to compute cluster centers using K-means\n",
        "def compute_cluster_centers(descriptors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(descriptors)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "# Function to compute histogram of visual words for an image\n",
        "def compute_histogram(descriptors, cluster_centers):\n",
        "    histogram = np.zeros(len(cluster_centers))\n",
        "    for descriptor in descriptors:\n",
        "        distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
        "        nearest_cluster = np.argmin(distances)\n",
        "        histogram[nearest_cluster] += 1\n",
        "    return histogram\n",
        "\n",
        "# # Load MNIST dataset\n",
        "# mnist = fetch_openml('mnist_784', version=1)\n",
        "# X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "def full(num_clusters):\n",
        "    # Initialize SIFT\n",
        "    X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # Initialize lists to store descriptors\n",
        "    descriptors_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        image = X_train[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        # descriptors_list.append(descriptors)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            descriptors_list.append(descriptors)\n",
        "            labels_list.append(y_train[i])  # Append corresponding label\n",
        "\n",
        "    # descriptors_list = [descriptors for descriptors in descriptors_list if descriptors is not None and descriptors.any()]\n",
        "\n",
        "    # Convert descriptors to a single numpy array\n",
        "    if descriptors_list:\n",
        "        descriptors_array = np.concatenate(descriptors_list, axis=0)\n",
        "\n",
        "        # Choose the number of clusters for K-means\n",
        "        num_clusters = num_clusters\n",
        "\n",
        "        # Compute cluster centers using K-means\n",
        "        cluster_centers = compute_cluster_centers(descriptors_array, num_clusters)\n",
        "\n",
        "        # Compute histograms of visual words for all images\n",
        "        t_histograms = []\n",
        "        t_labels = []\n",
        "        for descriptors, label in zip(descriptors_list, labels_list):\n",
        "            histogram = compute_histogram(descriptors, cluster_centers)\n",
        "            t_histograms.append(histogram)\n",
        "            t_labels.append(label)\n",
        "\n",
        "        # Convert histograms and labels to numpy arrays\n",
        "        X_train = np.array(t_histograms)\n",
        "        y_train = np.array(t_labels)\n",
        "\n",
        "        # Create a Linear SVM model\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
        "\n",
        "        # Train the SVM model\n",
        "        svm_model.fit(X_train, y_train)\n",
        "    else:\n",
        "        print(\"No descriptors found. Unable to create SVM model.\")\n",
        "\n",
        "    # Assuming you have a separate test set, similar to the training set\n",
        "    X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
        "\n",
        "    # Initialize lists to store test descriptors and labels\n",
        "    test_descriptors_list = []\n",
        "    test_labels_list = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        image = X_test[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            test_descriptors_list.append(descriptors)\n",
        "            test_labels_list.append(y_test[i])\n",
        "\n",
        "    # Compute histograms of visual words for all test images\n",
        "    test_histograms = []\n",
        "    test_labels = []\n",
        "    for descriptors, label in zip(test_descriptors_list, test_labels_list):\n",
        "        histogram = compute_histogram(descriptors, cluster_centers)\n",
        "        test_histograms.append(histogram)\n",
        "        test_labels.append(label)\n",
        "\n",
        "    # Convert test histograms and labels to numpy arrays\n",
        "    X_test = np.array(test_histograms)\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    # Make predictions using the trained SVM model\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy of the SVM model on the test set:\", accuracy)\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    filename = 'finalized_model_{}.sav'.format(num_clusters)\n",
        "    pickle.dump(svm_model, open(filename, 'wb'))\n",
        "\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHmkSCqzUdv",
        "outputId": "de717b23-fe50-4efe-ed91-716bb1daa669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.527062908496732\n",
            "Accuracy of the SVM model on the test set: 0.6453227124183006\n",
            "Accuracy of the SVM model on the test set: 0.7318218954248366\n",
            "Accuracy of the SVM model on the test set: 0.7918709150326797\n",
            "Accuracy of the SVM model on the test set: 0.8259803921568627\n",
            "Accuracy of the SVM model on the test set: 0.8393586601307189\n",
            "Accuracy of the SVM model on the test set: 0.8383374183006536\n"
          ]
        }
      ],
      "source": [
        "# clusters=[20,40,80,160,320,640,960,1280]\n",
        "clusters = [1280]\n",
        "accuracies=[]\n",
        "for num_clusters in clusters:\n",
        "  accuracy = full(num_clusters)\n",
        "  accuracies.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2wJ11bXNui60"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# filename = 'finalized_model_2.sav'\n",
        "# pickle.dump(svm_model, open(filename, 'wb'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pvBYppUpb1jT"
      },
      "outputs": [],
      "source": [
        "# # Assuming you have a separate test set, similar to the training set\n",
        "# X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
        "\n",
        "# # Initialize lists to store test descriptors and labels\n",
        "# test_descriptors_list = []\n",
        "# test_labels_list = []\n",
        "\n",
        "# for i in range(len(X_test)):\n",
        "#     image = X_test[i]\n",
        "#     keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "#     if descriptors is not None and descriptors.any():\n",
        "#         test_descriptors_list.append(descriptors)\n",
        "#         test_labels_list.append(y_test[i])\n",
        "\n",
        "# # Compute histograms of visual words for all test images\n",
        "# test_histograms = []\n",
        "# test_labels = []\n",
        "# for descriptors, label in zip(test_descriptors_list, test_labels_list):\n",
        "#     histogram = compute_histogram(descriptors, cluster_centers)\n",
        "#     test_histograms.append(histogram)\n",
        "#     test_labels.append(label)\n",
        "\n",
        "# # Convert test histograms and labels to numpy arrays\n",
        "# X_test = np.array(test_histograms)\n",
        "# y_test = np.array(test_labels)\n",
        "\n",
        "# # Make predictions using the trained SVM model\n",
        "# y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# # Calculate the accuracy of the model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy of the SVM model on the test set:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MIXtRUpu1QW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyNrzZcRb0dWmrsWRrTZX2lC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
