{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsasree/CV-Assignment-2/blob/main/CV_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WWTMbLVJatuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLDbm0bOgbgR",
        "outputId": "13f83692-3795-4661-9906-cc23a83f40b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 3656464.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1192280.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:02<00:00, 671654.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:01<00:00, 4099.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMrkNf5gfBe7",
        "outputId": "33956359-83d6-45ab-a6b8-10fd031ccb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mwgL67XfHEq",
        "outputId": "58f03419-fa08-46a3-ce8e-7d4a99907308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zRoKTznWK6Gn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Function to compute cluster centers using K-means\n",
        "def compute_cluster_centers(descriptors, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(descriptors)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "# Function to compute histogram of visual words for an image\n",
        "def compute_histogram(descriptors, cluster_centers):\n",
        "    histogram = np.zeros(len(cluster_centers))\n",
        "    for descriptor in descriptors:\n",
        "        distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
        "        nearest_cluster = np.argmin(distances)\n",
        "        histogram[nearest_cluster] += 1\n",
        "    return histogram\n",
        "\n",
        "# # Load MNIST dataset\n",
        "# mnist = fetch_openml('mnist_784', version=1)\n",
        "# X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "def create_svm_model(id):\n",
        "    if id == 1:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=0.1))\n",
        "    elif id == 2:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
        "    elif id == 3:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1.0, gamma='auto'))\n",
        "    elif id == 4:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='poly', degree=3, C=1.0))\n",
        "    elif id == 5:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10.0, gamma=0.1))\n",
        "    elif id == 6:\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='sigmoid', C=0.01, gamma=0.001))\n",
        "    else:\n",
        "        # Default model with linear kernel and default C value\n",
        "        svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
        "    \n",
        "    return svm_model\n",
        "\n",
        "def full(num_clusters,id):\n",
        "    # Initialize SIFT\n",
        "    X_train, y_train = train_set.data.numpy(), train_set.targets.numpy()\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # Initialize lists to store descriptors\n",
        "    descriptors_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        image = X_train[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        # descriptors_list.append(descriptors)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            descriptors_list.append(descriptors)\n",
        "            labels_list.append(y_train[i])  # Append corresponding label\n",
        "\n",
        "    # descriptors_list = [descriptors for descriptors in descriptors_list if descriptors is not None and descriptors.any()]\n",
        "\n",
        "    # Convert descriptors to a single numpy array\n",
        "    if descriptors_list:\n",
        "        descriptors_array = np.concatenate(descriptors_list, axis=0)\n",
        "\n",
        "        # Choose the number of clusters for K-means\n",
        "        num_clusters = num_clusters\n",
        "\n",
        "        # Compute cluster centers using K-means\n",
        "        cluster_centers = compute_cluster_centers(descriptors_array, num_clusters)\n",
        "\n",
        "        # Compute histograms of visual words for all images\n",
        "        t_histograms = []\n",
        "        t_labels = []\n",
        "        for descriptors, label in zip(descriptors_list, labels_list):\n",
        "            histogram = compute_histogram(descriptors, cluster_centers)\n",
        "            t_histograms.append(histogram)\n",
        "            t_labels.append(label)\n",
        "\n",
        "        # Convert histograms and labels to numpy arrays\n",
        "        X_train = np.array(t_histograms)\n",
        "        y_train = np.array(t_labels)\n",
        "\n",
        "        # Create a Linear SVM model\n",
        "        # if id == 1:\n",
        "        #     svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=0.1))\n",
        "        # elif if ==2:\n",
        "        #     svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
        "        svm_model = create_svm_model(id)\n",
        "        # Train the SVM model\n",
        "        svm_model.fit(X_train, y_train)\n",
        "    else:\n",
        "        print(\"No descriptors found. Unable to create SVM model.\")\n",
        "\n",
        "    # Assuming you have a separate test set, similar to the training set\n",
        "    X_test, y_test = test_set.data.numpy(), test_set.targets.numpy()\n",
        "\n",
        "    # Initialize lists to store test descriptors and labels\n",
        "    test_descriptors_list = []\n",
        "    test_labels_list = []\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        image = X_test[i]\n",
        "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "        if descriptors is not None and descriptors.any():\n",
        "            test_descriptors_list.append(descriptors)\n",
        "            test_labels_list.append(y_test[i])\n",
        "\n",
        "    # Compute histograms of visual words for all test images\n",
        "    test_histograms = []\n",
        "    test_labels = []\n",
        "    for descriptors, label in zip(test_descriptors_list, test_labels_list):\n",
        "        histogram = compute_histogram(descriptors, cluster_centers)\n",
        "        test_histograms.append(histogram)\n",
        "        test_labels.append(label)\n",
        "\n",
        "    # Convert test histograms and labels to numpy arrays\n",
        "    X_test = np.array(test_histograms)\n",
        "    y_test = np.array(test_labels)\n",
        "\n",
        "    # Make predictions using the trained SVM model\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy of the SVM model on the test set:\", accuracy)\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    filename = 'finalized_model_{}_{}.sav'.format(num_clusters,id)\n",
        "    pickle.dump(svm_model, open(filename, 'wb'))\n",
        "\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHmkSCqzUdv",
        "outputId": "8c38685e-525c-426a-aef7-9898e4c08ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM model on the test set: 0.7939133986928104\n",
            "Accuracy of the SVM model on the test set: 0.7953431372549019\n",
            "Accuracy of the SVM model on the test set: 0.8041258169934641\n",
            "Accuracy of the SVM model on the test set: 0.7609272875816994\n",
            "Accuracy of the SVM model on the test set: 0.29074754901960786\n",
            "Accuracy of the SVM model on the test set: 0.6537990196078431\n"
          ]
        }
      ],
      "source": [
        "# clusters=[10,20,40,80,160,320]\n",
        "clusters = [160]\n",
        "ids=[1,2,3,4,5,6]\n",
        "accuracies=[]\n",
        "for num_clusters in clusters:\n",
        "  for id in ids:\n",
        "    accuracy = full(num_clusters,id)\n",
        "    accuracies.append(accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
